{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cm_kogpt2_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dreamingjudith/KoGPT2-personachat/blob/dev/cm_kogpt2_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrzS4QP6-6IC",
        "colab_type": "text"
      },
      "source": [
        "# 사전 확인사항\n",
        "\n",
        "- Runtime type 변경 후 GPU가 제대로 할당됐는지 확인하기\n",
        "- 체크포인트 저장을 위한 Google Drive 연동\n",
        "- 필요 모듈 설치\n",
        "- 기존 코드에 남아있는 `logger`를 그대로 사용하기 위한 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuavZMbF2A22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "35db773b-f319-48a6-b73a-effeb66decf1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep  6 09:58:43 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL-JPUk52EYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44371d0a-d3dc-45f8-997f-87c0be0719cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n2cRZnfAJ9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b217662-3f65-4140-a87f-830773f251ed"
      },
      "source": [
        "!ls '/content/drive/My Drive'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'   KoGPT2-personachat\t korquad_2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V342LBue2RDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "07465010-d39e-4bfc-86ac-e960fae9b20b"
      },
      "source": [
        "!pip install gluonnlp mxnet pytorch-lightning sentencepiece transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.7.0.post1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.6.0+cu101)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (5.3.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: tensorboard==2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.31.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (49.6.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (3.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpypMJQADLgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger('cm_kogpt2')\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YPK0a7t-6Ig",
        "colab_type": "text"
      },
      "source": [
        "# KoGPT2 모델 다운받기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG40VVux2uWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import os\n",
        "import requests\n",
        "import sys\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer, SentencepieceDetokenizer\n",
        "from transformers import GPT2Config, GPT2LMHeadModel"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd1Z9FEG-6In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _download(url, filename, chksum, cachedir='~/kogpt2/'):\n",
        "    f_cachedir = os.path.expanduser(cachedir)\n",
        "    os.makedirs(f_cachedir, exist_ok=True)\n",
        "    file_path = os.path.join(f_cachedir, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        if hashlib.md5(open(file_path,\n",
        "                            'rb').read()).hexdigest()[:10] == chksum:\n",
        "            print('using cached model')\n",
        "            return file_path\n",
        "    with open(file_path, 'wb') as f:\n",
        "        response = requests.get(url, stream=True)\n",
        "        total = response.headers.get('content-length')\n",
        "\n",
        "        if total is None:\n",
        "            f.write(response.content)\n",
        "        else:\n",
        "            downloaded = 0\n",
        "            total = int(total)\n",
        "            for data in response.iter_content(\n",
        "                    chunk_size=max(int(total / 1000), 1024 * 1024)):\n",
        "                downloaded += len(data)\n",
        "                f.write(data)\n",
        "                done = int(50 * downloaded / total)\n",
        "                sys.stdout.write('\\r[{}{}]'.format('█' * done,\n",
        "                                                   '.' * (50 - done)))\n",
        "                sys.stdout.flush()\n",
        "    sys.stdout.write('\\n')\n",
        "    assert chksum == hashlib.md5(open(\n",
        "        file_path, 'rb').read()).hexdigest()[:10], 'corrupted file!'\n",
        "    return file_path\n",
        "\n",
        "\n",
        "def get_kogpt2_model(filepath, cachedir='/content/kogpt2/'):\n",
        "    \"\"\"Get KoGPT2 model after downloading\"\"\"\n",
        "\n",
        "    model_info = {\n",
        "        'url':\n",
        "        'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "        'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "        'chksum': '676e9bcfa7'\n",
        "    }\n",
        "\n",
        "    kogpt2_config = {\n",
        "        \"initializer_range\": 0.02,\n",
        "        \"layer_norm_epsilon\": 1e-05,\n",
        "        \"n_ctx\": 1024,\n",
        "        \"n_embd\": 768,\n",
        "        \"n_head\": 12,\n",
        "        \"n_layer\": 12,\n",
        "        \"n_positions\": 1024,\n",
        "        \"vocab_size\": 50000,\n",
        "        \"activation_function\": \"gelu\",\n",
        "        \"bos_id\": 0,\n",
        "        \"eos_id\": 1\n",
        "    }\n",
        "\n",
        "    if filepath:\n",
        "        logger.info(\"Loading {}\".format(filepath))\n",
        "        model_path = filepath\n",
        "    else:\n",
        "        model_path = _download(model_info['url'],\n",
        "                               model_info['fname'],\n",
        "                               model_info['chksum'],\n",
        "                               cachedir=cachedir)\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=None,\n",
        "                                            config=GPT2Config.from_dict(kogpt2_config),\n",
        "                                            state_dict=torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_kogpt2_tokenizer(cachedir='/content/kogpt2/'):\n",
        "    \"\"\"Get KoGPT2 Tokenizer after downloading\"\"\"\n",
        "\n",
        "    vocab_info = {\n",
        "        'url':\n",
        "        'https://kobert.blob.core.windows.net/models/kogpt2/tokenizer/kogpt2_news_wiki_ko_cased_818bfa919d.spiece',\n",
        "        'fname': 'kogpt2_news_wiki_ko_cased_818bfa919d.spiece',\n",
        "        'chksum': '818bfa919d'\n",
        "    }\n",
        "\n",
        "    vocab_path = _download(vocab_info['url'],\n",
        "                           vocab_info['fname'],\n",
        "                           vocab_info['chksum'],\n",
        "                           cachedir=cachedir)\n",
        "\n",
        "    tokenizer = SentencepieceTokenizer(vocab_path)\n",
        "    detokenizer = SentencepieceDetokenizer(vocab_path)\n",
        "    vocab = nlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "                                                   mask_token=None,\n",
        "                                                   sep_token=None,\n",
        "                                                   cls_token=None,\n",
        "                                                   unknown_token='<unk>',\n",
        "                                                   padding_token='<pad>',\n",
        "                                                   bos_token='<s>',\n",
        "                                                   eos_token='</s>')\n",
        "\n",
        "    return tokenizer, detokenizer, vocab"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H9UaMSk-6It",
        "colab_type": "text"
      },
      "source": [
        "# 데이터셋 로드 및 체크포인트 저장을 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ou2GYWy-6Iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from itertools import chain\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBo_k9nAEGM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPECIAL_TOKENS = [\"<s>\", \"</s>\", \"<usr>\", \"<sys>\", \"<pad>\"]\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<s>', 'eos_token': '</s>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<usr>', '<sys>']}\n",
        "MODEL_INPUTS = [\"input_ids\", \"labels\", \"token_type_ids\"]\n",
        "PADDED_INPUTS = [\"input_ids\", \"labels\", \"token_type_ids\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8IYXht-6I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(tokenizer, vocab, dataset_path, dataset_cache):\n",
        "    \"\"\"Read PersonaChat json file and return tokenized dataset\"\"\"\n",
        "    dataset_basename = os.path.basename(dataset_path).split(\".\")[0]\n",
        "    dataset_cache = \"/content/drive/My Drive/KoGPT2-personachat/dataset/dataset_cache_{}\".format(dataset_basename)\n",
        "\n",
        "    if dataset_cache and os.path.isfile(dataset_cache):\n",
        "        logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
        "        dataset = torch.load(dataset_cache)\n",
        "\n",
        "    else:\n",
        "        logger.info(\"Reading {}\".format(dataset_path))\n",
        "        with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            dataset = json.loads(f.read())\n",
        "\n",
        "        logger.info(\"Tokenize and encode the dataset\")\n",
        "\n",
        "        def tokenize(obj):\n",
        "            if isinstance(obj, str):\n",
        "                return vocab[tokenizer(obj)]\n",
        "            if isinstance(obj, dict):\n",
        "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "            return list(tokenize(o) for o in obj)\n",
        "        dataset = tokenize(dataset)\n",
        "        torch.save(dataset, dataset_cache)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def make_logdir(model_name: str):\n",
        "    \"\"\"Create unique path to save results and checkpoints, e.g. runs/Sep22_19-45-59_gpu-7_gpt2\"\"\"\n",
        "    # Code copied from ignite repo\n",
        "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "    logdir = os.path.join(\n",
        "        'runs', current_time + '_' + socket.gethostname() + '_' + model_name)\n",
        "    return logdir\n",
        "\n",
        "\n",
        "def pad_dataset(args, dataset, padding=0):\n",
        "    \"\"\" Pad the dataset.\n",
        "    This could be optimized by defining a Dataset class and padding at the batch level,\n",
        "    but this is simpler. \"\"\"\n",
        "    max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
        "\n",
        "    for name in PADDED_INPUTS:\n",
        "        dataset[name] = [x + [padding if name != \"labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def build_input_from_segments(persona, history, reply, vocab, labels=False, with_eos=True):\n",
        "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "    bos, eos, speaker1, speaker2 = vocab[SPECIAL_TOKENS[:-1]]\n",
        "    sequence = [[bos] + list(chain(*persona))] + \\\n",
        "        history + [reply + ([eos] if with_eos else [])]\n",
        "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) %\n",
        "                                 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
        "    instance = {}\n",
        "    instance[\"input_ids\"] = list(chain(*sequence))\n",
        "    instance[\"token_type_ids\"] = [speaker2 if i %\n",
        "                                  2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "    instance[\"labels\"] = [-100] * len(instance[\"input_ids\"])\n",
        "    if labels:\n",
        "        instance[\"labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
        "\n",
        "    return instance\n",
        "\n",
        "\n",
        "def get_data_loaders(args, tokenizer, vocab):\n",
        "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
        "    personachat = get_dataset(tokenizer, vocab, args.dataset_path, args.dataset_cache)\n",
        "\n",
        "    logger.info(\"Build inputs and labels\")\n",
        "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
        "    for dataset_name, dataset in personachat.items():\n",
        "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
        "        if args.num_candidates > 0 and dataset_name == 'train':\n",
        "            num_candidates = min(args.num_candidates, num_candidates)\n",
        "        for dialog in dataset:\n",
        "            persona = dialog[\"personality\"].copy()\n",
        "            for _ in range(args.personality_permutations):\n",
        "                for utterance in dialog[\"utterances\"]:\n",
        "                    history = utterance[\"history\"][-(2 * args.max_history + 1):]\n",
        "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
        "                        labels = bool(j == num_candidates - 1)\n",
        "                        instance = build_input_from_segments(\n",
        "                            persona, history, candidate, vocab, labels)\n",
        "                        for input_name, input_array in instance.items():\n",
        "                            datasets[dataset_name][input_name].append(\n",
        "                                input_array)\n",
        "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
        "                # permuted personalities\n",
        "                persona = [persona[-1]] + persona[:-1]\n",
        "\n",
        "    logger.info(\"Pad inputs and convert to Tensor\")\n",
        "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "        dataset = pad_dataset(\n",
        "            args, dataset, padding=vocab[SPECIAL_TOKENS[-1]])\n",
        "        for input_name in MODEL_INPUTS:\n",
        "            tensor = torch.tensor(dataset[input_name])\n",
        "            tensor = tensor.view(\n",
        "                (-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
        "            tensor_datasets[dataset_name].append(tensor)\n",
        "\n",
        "    logger.info(\"Build train and validation dataloaders\")\n",
        "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=args.train_batch_size,\n",
        "                              num_workers=args.num_workers,\n",
        "                              shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=args.valid_batch_size,\n",
        "                              num_workers=args.num_workers,\n",
        "                              shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcF2l7---6I4",
        "colab_type": "text"
      },
      "source": [
        "# PersonaChat model defined for PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GirV94Yy-6I5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJf-tQlS-6I-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CMPersonaChat(LightningModule):\n",
        "    def __init__(self, hparams, *args):\n",
        "        super(CMPersonaChat, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        self.kogpt2 = get_kogpt2_model(hparams.model_params)\n",
        "\n",
        "    def forward(self, inputs, token_type_ids):\n",
        "        output, *_ = self.kogpt2(inputs, token_type_ids=token_type_ids)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        batch = tuple(input_tensor.to(self.hparams.device) for input_tensor in batch)\n",
        "        token_ids, label, mask = batch\n",
        "        # forward: input(batch,max_sentence_length) -> output(batch_size, max_sentence_length,vocab)\n",
        "        # e.g. (4,768) -> (4,768,50000)\n",
        "        loss, *_ = self.kogpt2(token_ids, token_type_ids=mask, labels=label)\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        batch = tuple(input_tensor.to(self.hparams.device) for input_tensor in batch)\n",
        "        token_ids, label, mask = batch\n",
        "        loss, *_ = self.kogpt2(token_ids, token_type_ids=mask, labels=label)\n",
        "        return {'val_loss': loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'val_loss': avg_loss}\n",
        "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Prepare optimizer\n",
        "        param_optimizer = list(self.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                          lr=self.hparams.lr, correct_bias=False)\n",
        "        # warm up lr\n",
        "        num_train_steps = len(self.train_dataloader()) * self.hparams.max_epochs\n",
        "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
        "        lr_scheduler = {'scheduler': scheduler, 'name': 'cosine_schedule_with_warmup',\n",
        "                        'monitor': 'loss', 'interval': 'step',\n",
        "                        'frequency': 1}\n",
        "        return [optimizer], [lr_scheduler]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uf6yPSc-6JD",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fLdH4L_-6JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG6hjs73-6JI",
        "colab_type": "text"
      },
      "source": [
        "## argparser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy7oJ1fn-6JK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0af90549-9e05-4a0b-e32e-56d041879a2d"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--device\", type=str,\n",
        "                    default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "                    help=\"Device (cuda or cpu)\")\n",
        "parser.add_argument(\"--dataset_path\", type=str,\n",
        "                    default=\"dataset/personachat_manual_translated.json\",\n",
        "                    help=\"Path of the dataset.\")\n",
        "parser.add_argument(\"--dataset_cache\", type=str,\n",
        "                    default='./dataset_cache',\n",
        "                    help=\"Path or url of the dataset cache\")\n",
        "parser.add_argument(\"--num_candidates\", type=int, default=1,\n",
        "                    help=\"Number of candidates for training\")\n",
        "parser.add_argument(\"--personality_permutations\", type=int, default=1,\n",
        "                    help=\"Number of permutations of personality sentences\")\n",
        "parser.add_argument(\"--max_history\", type=int, default=1,\n",
        "                    help=\"Number of previous exchanges to keep in history\")\n",
        "parser.add_argument(\"--name\", type=str,\n",
        "                    default=\"cm_kogpt2\",\n",
        "                    help=\"Model name for logging\")\n",
        "parser.add_argument('--lr',\n",
        "                    type=float,\n",
        "                    default=5e-5,\n",
        "                    help='The initial learning rate')\n",
        "parser.add_argument('--warmup_ratio',\n",
        "                    type=float,\n",
        "                    default=0.1,\n",
        "                    help='warmup ratio')\n",
        "\n",
        "# Shared arguments for dataloader and training\n",
        "parser.add_argument('--max_len',\n",
        "                    type=int,\n",
        "                    default=768,\n",
        "                    help='max sentence length on input (default: 768)')\n",
        "parser.add_argument(\"--train_batch_size\", type=int,\n",
        "                    default=2, help=\"Batch size for training\")\n",
        "parser.add_argument(\"--valid_batch_size\", type=int,\n",
        "                    default=1, help=\"Batch size for validation\")\n",
        "parser.add_argument(\"--num_workers\", type=int,\n",
        "                    default=16, help=\"Number of workers for DataLoader\")\n",
        "\n",
        "# Select train/inference\n",
        "parser.add_argument('--train',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='eval train set (default: False)')\n",
        "parser.add_argument('--restore',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='train using saved checkpoint (default: False)')\n",
        "parser.add_argument('--chat',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='response generation on given user input')\n",
        "parser.add_argument('--model_params',\n",
        "                    type=str,\n",
        "                    help='model binary for starting chat')\n",
        "\n",
        "# Additional arguments for chatting\n",
        "parser.add_argument(\"--temperature\", type=float, default=0.7, help=\"Sampling softmax temperature\")\n",
        "parser.add_argument(\"--top_k\", type=int, default=0, help=\"Filter top-k tokens before sampling (<=0: no filtering)\")\n",
        "parser.add_argument(\"--top_p\", type=float, default=0.9, help=\"Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\")\n",
        "parser.add_argument(\"--no_sample\", action='store_true', help=\"Set to use greedy decoding instead of sampling\")\n",
        "parser.add_argument(\"--min_length\", type=int, default=1, help=\"Minimum length of the output utterances\")\n",
        "\n",
        "# Evaluation\n",
        "parser.add_argument('--chat_test',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='response generation on given user input')\n",
        "parser.add_argument(\"--eval_dataset_path\", type=str,\n",
        "                    default=\"eval/eval_merge.json\",\n",
        "                    help=\"Path of the evaluation dataset.\")\n",
        "parser.add_argument('--num_eval_pp',\n",
        "                    type=int, default=10,\n",
        "                    help='The number of dialogue steps for the ping-pong test ')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--num_eval_pp'], dest='num_eval_pp', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, help='The number of dialogue steps for the ping-pong test ', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdj3ECt9cllF",
        "colab_type": "text"
      },
      "source": [
        "## main function\n",
        "\n",
        "이 때 Colab에서 argument가 정상적으로 들어가게 하기 위해 아래와 같은 방식으로 `parse_args` 함수에 인자로 `args` 리스트를 줘야 힘\n",
        "```\n",
        "args = parser.parse_args(args=['--train', '--dataset_path', '/content/drive/My Drive/KoGPT2-personachat/dataset/sample.json'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFl0wsz3-6JO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "bb9f8d2b-7bfa-4036-d7ef-3808c8da2667"
      },
      "source": [
        "# Model configuration augments\n",
        "parser = Trainer.add_argparse_args(parser)\n",
        "args = parser.parse_args(args=['--train', '--dataset_path', '/content/drive/My Drive/KoGPT2-personachat/dataset/personachat_google_translated.json'])\n",
        "\n",
        "tokenizer, detokenizer, vocab = get_kogpt2_tokenizer()\n",
        "model = CMPersonaChat(args)\n",
        "model.to(args.device)\n",
        "\n",
        "# Fine-tuning KoGPT2 for the PersonaChat\n",
        "train_loader, val_loader = get_data_loaders(args, tokenizer, vocab)\n",
        "tb_logger = TensorBoardLogger(\"/content/drive/My Drive/KoGPT2-personachat/logs\", name=args.name)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='{}/checkpoints/{}'.format(tb_logger.log_dir, '{epoch:02d}-{val_loss:.4f}'),\n",
        "    verbose=True,\n",
        "    save_last=True,\n",
        "    save_top_k=10,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    prefix='model_'\n",
        ")\n",
        "\n",
        "if args.restore:\n",
        "    trainer = Trainer(resume_from_checkpoint=args.model_params,\n",
        "                        checkpoint_callback=checkpoint_callback,\n",
        "                        gradient_clip_val=1.0,\n",
        "                        logger=tb_logger)\n",
        "else:\n",
        "    trainer = Trainer.from_argparse_args(\n",
        "        args,\n",
        "        checkpoint_callback=checkpoint_callback,\n",
        "        weights_save_path=os.getcwd(),\n",
        "        gradient_clip_val=1.0,\n",
        "        logger=tb_logger)\n",
        "model.train()\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "logging.info('best model path {}'.format(checkpoint_callback.best_model_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:cm_kogpt2:Load tokenized dataset from cache at /content/drive/My Drive/KoGPT2-personachat/dataset/dataset_cache_personachat_google_translated\n",
            "INFO:cm_kogpt2:Build inputs and labels\n",
            "INFO:cm_kogpt2:Pad inputs and convert to Tensor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9oMCvgjdPsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}