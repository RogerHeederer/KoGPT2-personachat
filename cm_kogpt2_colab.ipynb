{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cm_kogpt2_colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a35ed21a3b8410385c4af3fc0dc976d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0d38cc8992df438fa8a48578db41e53f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a98e41b25374d7a832bd6beff97e88b",
              "IPY_MODEL_37990756de3f4c27aa0817fcf7ae0145"
            ]
          }
        },
        "0d38cc8992df438fa8a48578db41e53f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "5a98e41b25374d7a832bd6beff97e88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3e7ad3e7411a48d19f04665a2477198d",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_291d2b45f2f947268ed5da04bcb5c91f"
          }
        },
        "37990756de3f4c27aa0817fcf7ae0145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8f66f946411043ec9673383018848abd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1.0 [00:01&lt;00:00,  1.48it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af5ae3ebcb7b4cf79b98179615bea3ef"
          }
        },
        "3e7ad3e7411a48d19f04665a2477198d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "291d2b45f2f947268ed5da04bcb5c91f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f66f946411043ec9673383018848abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af5ae3ebcb7b4cf79b98179615bea3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "975ab6a2699942b3b542e32f5a558526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2cd78e68f51a4e94a082dcd1d07ae1b8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_32521e3fc51246b094bbda6f265620e6",
              "IPY_MODEL_fb83bce3df9d4280b4b1d1ce25ac7fd3"
            ]
          }
        },
        "2cd78e68f51a4e94a082dcd1d07ae1b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "32521e3fc51246b094bbda6f265620e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1c059b0e7f2547f1956d77b53feaa0c6",
            "_dom_classes": [],
            "description": "Epoch 1:  94%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 435,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 410,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_808e11ff80fd45a8bfd4e4d5a7823857"
          }
        },
        "fb83bce3df9d4280b4b1d1ce25ac7fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a1d6800b4c24420986bf54e4261831bf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 410/435 [02:19&lt;00:08,  2.94it/s, loss=3.628, v_num=1]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acc03c77e80449eebb7da5f448da28a6"
          }
        },
        "1c059b0e7f2547f1956d77b53feaa0c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "808e11ff80fd45a8bfd4e4d5a7823857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1d6800b4c24420986bf54e4261831bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acc03c77e80449eebb7da5f448da28a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "30c62ef5a7f146749f3b629b762ad744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de768d79f8fa4f44bd524abe77c95b02",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3732fbff36a146889bccd853a62c7eca",
              "IPY_MODEL_d2298f1b329148588454ff5e8cbdce30"
            ]
          }
        },
        "de768d79f8fa4f44bd524abe77c95b02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "3732fbff36a146889bccd853a62c7eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_805d8e349617450e8db420ccdb126184",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e54ac942cf44cb48bf496cc0a0ecc88"
          }
        },
        "d2298f1b329148588454ff5e8cbdce30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fe96b85a23d44bfc9d1e10a07084d213",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 72/72 [01:45&lt;00:00,  1.29it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_113d48a9071240f3a697edcd63bb33c2"
          }
        },
        "805d8e349617450e8db420ccdb126184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e54ac942cf44cb48bf496cc0a0ecc88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe96b85a23d44bfc9d1e10a07084d213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "113d48a9071240f3a697edcd63bb33c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "253d0fb8003e4200911e26c5d71a01f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7981d9aa530b4d4d9432e9cb445f4de8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a79f0a017bc4cd6bb58175f3dd3c60e",
              "IPY_MODEL_662ec579b98142eb81a167c44e3a13df"
            ]
          }
        },
        "7981d9aa530b4d4d9432e9cb445f4de8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "5a79f0a017bc4cd6bb58175f3dd3c60e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f69fe7cd4534154a84076e3b08f3e0c",
            "_dom_classes": [],
            "description": "Validating:  65%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e18ab399ecce416390b8236c06e59592"
          }
        },
        "662ec579b98142eb81a167c44e3a13df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_535dbac28ad746afacfac20ea6e82dac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 47/72 [00:36&lt;00:19,  1.29it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8eeaf9cd5b024132a79ea8596f762cd8"
          }
        },
        "6f69fe7cd4534154a84076e3b08f3e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e18ab399ecce416390b8236c06e59592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "535dbac28ad746afacfac20ea6e82dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8eeaf9cd5b024132a79ea8596f762cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrzS4QP6-6IC",
        "colab_type": "text"
      },
      "source": [
        "# 사전 확인사항\n",
        "\n",
        "- Runtime type 변경 후 GPU가 제대로 할당됐는지 확인하기\n",
        "- 체크포인트 저장을 위한 Google Drive 연동\n",
        "- 필요 모듈 설치\n",
        "- 기존 코드에 남아있는 `logger`를 그대로 사용하기 위한 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuavZMbF2A22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "3ed82d99-65fd-47db-a4cc-5a728b736a7d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep  6 06:19:28 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    36W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL-JPUk52EYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2fea6db1-a5fa-4f95-baea-76378a8709fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n2cRZnfAJ9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f73fe8e0-3f33-48a4-9127-7efb6ef3d95d"
      },
      "source": [
        "!ls '/content/drive/My Drive'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'   KoGPT2-personachat\t korquad_2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V342LBue2RDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "1f2d157d-7a9b-4cd3-ffcb-2de11384e97b"
      },
      "source": [
        "!pip install gluonnlp mxnet pytorch-lightning sentencepiece transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.6/dist-packages (1.7.0.post1)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (20.4)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp) (0.29.21)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (5.3.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.6.0+cu101)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: tensorboard==2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.2.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (49.6.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (1.31.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpypMJQADLgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger('cm_kogpt2')\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YPK0a7t-6Ig",
        "colab_type": "text"
      },
      "source": [
        "# KoGPT2 모델 다운받기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG40VVux2uWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import os\n",
        "import requests\n",
        "import sys\n",
        "\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer, SentencepieceDetokenizer\n",
        "from transformers import GPT2Config, GPT2LMHeadModel"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd1Z9FEG-6In",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _download(url, filename, chksum, cachedir='~/kogpt2/'):\n",
        "    f_cachedir = os.path.expanduser(cachedir)\n",
        "    os.makedirs(f_cachedir, exist_ok=True)\n",
        "    file_path = os.path.join(f_cachedir, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        if hashlib.md5(open(file_path,\n",
        "                            'rb').read()).hexdigest()[:10] == chksum:\n",
        "            print('using cached model')\n",
        "            return file_path\n",
        "    with open(file_path, 'wb') as f:\n",
        "        response = requests.get(url, stream=True)\n",
        "        total = response.headers.get('content-length')\n",
        "\n",
        "        if total is None:\n",
        "            f.write(response.content)\n",
        "        else:\n",
        "            downloaded = 0\n",
        "            total = int(total)\n",
        "            for data in response.iter_content(\n",
        "                    chunk_size=max(int(total / 1000), 1024 * 1024)):\n",
        "                downloaded += len(data)\n",
        "                f.write(data)\n",
        "                done = int(50 * downloaded / total)\n",
        "                sys.stdout.write('\\r[{}{}]'.format('█' * done,\n",
        "                                                   '.' * (50 - done)))\n",
        "                sys.stdout.flush()\n",
        "    sys.stdout.write('\\n')\n",
        "    assert chksum == hashlib.md5(open(\n",
        "        file_path, 'rb').read()).hexdigest()[:10], 'corrupted file!'\n",
        "    return file_path\n",
        "\n",
        "\n",
        "def get_kogpt2_model(filepath, cachedir='/content/kogpt2/'):\n",
        "    \"\"\"Get KoGPT2 model after downloading\"\"\"\n",
        "\n",
        "    model_info = {\n",
        "        'url':\n",
        "        'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "        'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "        'chksum': '676e9bcfa7'\n",
        "    }\n",
        "\n",
        "    kogpt2_config = {\n",
        "        \"initializer_range\": 0.02,\n",
        "        \"layer_norm_epsilon\": 1e-05,\n",
        "        \"n_ctx\": 1024,\n",
        "        \"n_embd\": 768,\n",
        "        \"n_head\": 12,\n",
        "        \"n_layer\": 12,\n",
        "        \"n_positions\": 1024,\n",
        "        \"vocab_size\": 50000,\n",
        "        \"activation_function\": \"gelu\",\n",
        "        \"bos_id\": 0,\n",
        "        \"eos_id\": 1\n",
        "    }\n",
        "\n",
        "    if filepath:\n",
        "        logger.info(\"Loading {}\".format(filepath))\n",
        "        model_path = filepath\n",
        "    else:\n",
        "        model_path = _download(model_info['url'],\n",
        "                               model_info['fname'],\n",
        "                               model_info['chksum'],\n",
        "                               cachedir=cachedir)\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=None,\n",
        "                                            config=GPT2Config.from_dict(kogpt2_config),\n",
        "                                            state_dict=torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_kogpt2_tokenizer(cachedir='/content/kogpt2/'):\n",
        "    \"\"\"Get KoGPT2 Tokenizer after downloading\"\"\"\n",
        "\n",
        "    vocab_info = {\n",
        "        'url':\n",
        "        'https://kobert.blob.core.windows.net/models/kogpt2/tokenizer/kogpt2_news_wiki_ko_cased_818bfa919d.spiece',\n",
        "        'fname': 'kogpt2_news_wiki_ko_cased_818bfa919d.spiece',\n",
        "        'chksum': '818bfa919d'\n",
        "    }\n",
        "\n",
        "    vocab_path = _download(vocab_info['url'],\n",
        "                           vocab_info['fname'],\n",
        "                           vocab_info['chksum'],\n",
        "                           cachedir=cachedir)\n",
        "\n",
        "    tokenizer = SentencepieceTokenizer(vocab_path)\n",
        "    detokenizer = SentencepieceDetokenizer(vocab_path)\n",
        "    vocab = nlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "                                                   mask_token=None,\n",
        "                                                   sep_token=None,\n",
        "                                                   cls_token=None,\n",
        "                                                   unknown_token='<unk>',\n",
        "                                                   padding_token='<pad>',\n",
        "                                                   bos_token='<s>',\n",
        "                                                   eos_token='</s>')\n",
        "\n",
        "    return tokenizer, detokenizer, vocab"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H9UaMSk-6It",
        "colab_type": "text"
      },
      "source": [
        "# 데이터셋 로드 및 체크포인트 저장을 위한 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ou2GYWy-6Iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from itertools import chain\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBo_k9nAEGM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPECIAL_TOKENS = [\"<s>\", \"</s>\", \"<usr>\", \"<sys>\", \"<pad>\"]\n",
        "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<s>', 'eos_token': '</s>', 'pad_token': '<pad>',\n",
        "                         'additional_special_tokens': ['<usr>', '<sys>']}\n",
        "MODEL_INPUTS = [\"input_ids\", \"labels\", \"token_type_ids\"]\n",
        "PADDED_INPUTS = [\"input_ids\", \"labels\", \"token_type_ids\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju8IYXht-6I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(tokenizer, vocab, dataset_path, dataset_cache):\n",
        "    \"\"\"Read PersonaChat json file and return tokenized dataset\"\"\"\n",
        "    dataset_basename = os.path.basename(dataset_path).split(\".\")[0]\n",
        "    dataset_cache = \"dataset_cache_{}\".format(dataset_basename)\n",
        "\n",
        "    if dataset_cache and os.path.isfile(dataset_cache):\n",
        "        logger.info(\"Load tokenized dataset from cache at %s\", dataset_cache)\n",
        "        dataset = torch.load(dataset_cache)\n",
        "\n",
        "    else:\n",
        "        logger.info(\"Reading {}\".format(dataset_path))\n",
        "        with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            dataset = json.loads(f.read())\n",
        "\n",
        "        logger.info(\"Tokenize and encode the dataset\")\n",
        "\n",
        "        def tokenize(obj):\n",
        "            if isinstance(obj, str):\n",
        "                return vocab[tokenizer(obj)]\n",
        "            if isinstance(obj, dict):\n",
        "                return dict((n, tokenize(o)) for n, o in obj.items())\n",
        "            return list(tokenize(o) for o in obj)\n",
        "        dataset = tokenize(dataset)\n",
        "        torch.save(dataset, dataset_cache)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def make_logdir(model_name: str):\n",
        "    \"\"\"Create unique path to save results and checkpoints, e.g. runs/Sep22_19-45-59_gpu-7_gpt2\"\"\"\n",
        "    # Code copied from ignite repo\n",
        "    current_time = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "    logdir = os.path.join(\n",
        "        'runs', current_time + '_' + socket.gethostname() + '_' + model_name)\n",
        "    return logdir\n",
        "\n",
        "\n",
        "def pad_dataset(args, dataset, padding=0):\n",
        "    \"\"\" Pad the dataset.\n",
        "    This could be optimized by defining a Dataset class and padding at the batch level,\n",
        "    but this is simpler. \"\"\"\n",
        "    # max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
        "    max_l = args.max_len\n",
        "\n",
        "    for name in PADDED_INPUTS:\n",
        "        dataset[name] = [x + [padding if name != \"labels\" else -100] * (max_l - len(x)) for x in dataset[name]]\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def build_input_from_segments(persona, history, reply, vocab, labels=False, with_eos=True):\n",
        "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
        "    bos, eos, speaker1, speaker2 = vocab[SPECIAL_TOKENS[:-1]]\n",
        "    sequence = [[bos] + list(chain(*persona))] + \\\n",
        "        history + [reply + ([eos] if with_eos else [])]\n",
        "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) %\n",
        "                                 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
        "    instance = {}\n",
        "    instance[\"input_ids\"] = list(chain(*sequence))\n",
        "    instance[\"token_type_ids\"] = [speaker2 if i %\n",
        "                                  2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
        "    instance[\"labels\"] = [-100] * len(instance[\"input_ids\"])\n",
        "    if labels:\n",
        "        instance[\"labels\"] = ([-100] * sum(len(s) for s in sequence[:-1])) + [-100] + sequence[-1][1:]\n",
        "\n",
        "    return instance\n",
        "\n",
        "\n",
        "def get_data_loaders(args, tokenizer, vocab):\n",
        "    \"\"\" Prepare the dataset for training and evaluation \"\"\"\n",
        "    personachat = get_dataset(tokenizer, vocab, args.dataset_path, args.dataset_cache)\n",
        "\n",
        "    logger.info(\"Build inputs and labels\")\n",
        "    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\n",
        "    for dataset_name, dataset in personachat.items():\n",
        "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
        "        if args.num_candidates > 0 and dataset_name == 'train':\n",
        "            num_candidates = min(args.num_candidates, num_candidates)\n",
        "        for dialog in dataset:\n",
        "            persona = dialog[\"personality\"].copy()\n",
        "            for _ in range(args.personality_permutations):\n",
        "                for utterance in dialog[\"utterances\"]:\n",
        "                    history = utterance[\"history\"][-(2 * args.max_history + 1):]\n",
        "                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\n",
        "                        labels = bool(j == num_candidates - 1)\n",
        "                        instance = build_input_from_segments(\n",
        "                            persona, history, candidate, vocab, labels)\n",
        "                        for input_name, input_array in instance.items():\n",
        "                            datasets[dataset_name][input_name].append(\n",
        "                                input_array)\n",
        "                    datasets[dataset_name][\"n_candidates\"] = num_candidates\n",
        "                # permuted personalities\n",
        "                persona = [persona[-1]] + persona[:-1]\n",
        "\n",
        "    logger.info(\"Pad inputs and convert to Tensor\")\n",
        "    tensor_datasets = {\"train\": [], \"valid\": []}\n",
        "    for dataset_name, dataset in datasets.items():\n",
        "        dataset = pad_dataset(\n",
        "            args, dataset, padding=vocab[SPECIAL_TOKENS[-1]])\n",
        "        for input_name in MODEL_INPUTS:\n",
        "            tensor = torch.tensor(dataset[input_name])\n",
        "            tensor = tensor.view(\n",
        "                (-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\n",
        "            tensor_datasets[dataset_name].append(tensor)\n",
        "\n",
        "    logger.info(\"Build train and validation dataloaders\")\n",
        "    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=args.train_batch_size,\n",
        "                              num_workers=args.num_workers,\n",
        "                              shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=args.valid_batch_size,\n",
        "                              num_workers=args.num_workers,\n",
        "                              shuffle=False)\n",
        "\n",
        "    return train_loader, valid_loader"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcF2l7---6I4",
        "colab_type": "text"
      },
      "source": [
        "# PersonaChat model defined for PyTorch Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GirV94Yy-6I5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJf-tQlS-6I-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CMPersonaChat(LightningModule):\n",
        "    def __init__(self, hparams, *args):\n",
        "        super(CMPersonaChat, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        self.kogpt2 = get_kogpt2_model(hparams.model_params)\n",
        "\n",
        "    def forward(self, inputs, token_type_ids):\n",
        "        output, *_ = self.kogpt2(inputs, token_type_ids=token_type_ids)\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        batch = tuple(input_tensor.to(self.hparams.device) for input_tensor in batch)\n",
        "        token_ids, label, mask = batch\n",
        "        # forward: input(batch,max_sentence_length) -> output(batch_size, max_sentence_length,vocab)\n",
        "        # e.g. (4,768) -> (4,768,50000)\n",
        "        loss, *_ = self.kogpt2(token_ids, token_type_ids=mask, labels=label)\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        batch = tuple(input_tensor.to(self.hparams.device) for input_tensor in batch)\n",
        "        token_ids, label, mask = batch\n",
        "        loss, *_ = self.kogpt2(token_ids, token_type_ids=mask, labels=label)\n",
        "        return {'val_loss': loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'val_loss': avg_loss}\n",
        "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Prepare optimizer\n",
        "        param_optimizer = list(self.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                          lr=self.hparams.lr, correct_bias=False)\n",
        "        # warm up lr\n",
        "        num_train_steps = len(self.train_dataloader()) * self.hparams.max_epochs\n",
        "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
        "        lr_scheduler = {'scheduler': scheduler, 'name': 'cosine_schedule_with_warmup',\n",
        "                        'monitor': 'loss', 'interval': 'step',\n",
        "                        'frequency': 1}\n",
        "        return [optimizer], [lr_scheduler]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uf6yPSc-6JD",
        "colab_type": "text"
      },
      "source": [
        "# Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fLdH4L_-6JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG6hjs73-6JI",
        "colab_type": "text"
      },
      "source": [
        "## argparser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy7oJ1fn-6JK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "46e947e1-7db2-43b4-9adc-894424e33f68"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--device\", type=str,\n",
        "                    default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "                    help=\"Device (cuda or cpu)\")\n",
        "parser.add_argument(\"--dataset_path\", type=str,\n",
        "                    default=\"dataset/personachat_manual_translated.json\",\n",
        "                    help=\"Path of the dataset.\")\n",
        "parser.add_argument(\"--dataset_cache\", type=str,\n",
        "                    default='./dataset_cache',\n",
        "                    help=\"Path or url of the dataset cache\")\n",
        "parser.add_argument(\"--num_candidates\", type=int, default=1,\n",
        "                    help=\"Number of candidates for training\")\n",
        "parser.add_argument(\"--personality_permutations\", type=int, default=1,\n",
        "                    help=\"Number of permutations of personality sentences\")\n",
        "parser.add_argument(\"--max_history\", type=int, default=2,\n",
        "                    help=\"Number of previous exchanges to keep in history\")\n",
        "parser.add_argument(\"--name\", type=str,\n",
        "                    default=\"cm_kogpt2\",\n",
        "                    help=\"Model name for logging\")\n",
        "parser.add_argument('--lr',\n",
        "                    type=float,\n",
        "                    default=5e-5,\n",
        "                    help='The initial learning rate')\n",
        "parser.add_argument('--warmup_ratio',\n",
        "                    type=float,\n",
        "                    default=0.1,\n",
        "                    help='warmup ratio')\n",
        "\n",
        "# Shared arguments for dataloader and training\n",
        "parser.add_argument('--max_len',\n",
        "                    type=int,\n",
        "                    default=768,\n",
        "                    help='max sentence length on input (default: 768)')\n",
        "parser.add_argument(\"--train_batch_size\", type=int,\n",
        "                    default=2, help=\"Batch size for training\")\n",
        "parser.add_argument(\"--valid_batch_size\", type=int,\n",
        "                    default=1, help=\"Batch size for validation\")\n",
        "parser.add_argument(\"--num_workers\", type=int,\n",
        "                    default=16, help=\"Number of workers for DataLoader\")\n",
        "\n",
        "# Select train/inference\n",
        "parser.add_argument('--train',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='eval train set (default: False)')\n",
        "parser.add_argument('--restore',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='train using saved checkpoint (default: False)')\n",
        "parser.add_argument('--chat',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='response generation on given user input')\n",
        "parser.add_argument('--model_params',\n",
        "                    type=str,\n",
        "                    help='model binary for starting chat')\n",
        "\n",
        "# Additional arguments for chatting\n",
        "parser.add_argument(\"--temperature\", type=float, default=0.7, help=\"Sampling softmax temperature\")\n",
        "parser.add_argument(\"--top_k\", type=int, default=0, help=\"Filter top-k tokens before sampling (<=0: no filtering)\")\n",
        "parser.add_argument(\"--top_p\", type=float, default=0.9, help=\"Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\")\n",
        "parser.add_argument(\"--no_sample\", action='store_true', help=\"Set to use greedy decoding instead of sampling\")\n",
        "parser.add_argument(\"--min_length\", type=int, default=1, help=\"Minimum length of the output utterances\")\n",
        "\n",
        "# Evaluation\n",
        "parser.add_argument('--chat_test',\n",
        "                    action='store_true',\n",
        "                    default=False,\n",
        "                    help='response generation on given user input')\n",
        "parser.add_argument(\"--eval_dataset_path\", type=str,\n",
        "                    default=\"eval/eval_merge.json\",\n",
        "                    help=\"Path of the evaluation dataset.\")\n",
        "parser.add_argument('--num_eval_pp',\n",
        "                    type=int, default=10,\n",
        "                    help='The number of dialogue steps for the ping-pong test ')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--num_eval_pp'], dest='num_eval_pp', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, help='The number of dialogue steps for the ping-pong test ', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## main function\n",
        "\n",
        "이 때 Colab에서 argument가 정상적으로 들어가게 하기 위해 아래와 같은 방식으로 `parse_args` 함수에 인자로 `args` 리스트를 줘야 힘\n",
        "```\n",
        "args = parser.parse_args(args=['--train', '--dataset_path', '/content/drive/My Drive/KoGPT2-personachat/dataset/sample.json'])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFl0wsz3-6JO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587,
          "referenced_widgets": [
            "3a35ed21a3b8410385c4af3fc0dc976d",
            "0d38cc8992df438fa8a48578db41e53f",
            "5a98e41b25374d7a832bd6beff97e88b",
            "37990756de3f4c27aa0817fcf7ae0145",
            "3e7ad3e7411a48d19f04665a2477198d",
            "291d2b45f2f947268ed5da04bcb5c91f",
            "8f66f946411043ec9673383018848abd",
            "af5ae3ebcb7b4cf79b98179615bea3ef",
            "975ab6a2699942b3b542e32f5a558526",
            "2cd78e68f51a4e94a082dcd1d07ae1b8",
            "32521e3fc51246b094bbda6f265620e6",
            "fb83bce3df9d4280b4b1d1ce25ac7fd3",
            "1c059b0e7f2547f1956d77b53feaa0c6",
            "808e11ff80fd45a8bfd4e4d5a7823857",
            "a1d6800b4c24420986bf54e4261831bf",
            "acc03c77e80449eebb7da5f448da28a6",
            "30c62ef5a7f146749f3b629b762ad744",
            "de768d79f8fa4f44bd524abe77c95b02",
            "3732fbff36a146889bccd853a62c7eca",
            "d2298f1b329148588454ff5e8cbdce30",
            "805d8e349617450e8db420ccdb126184",
            "2e54ac942cf44cb48bf496cc0a0ecc88",
            "fe96b85a23d44bfc9d1e10a07084d213",
            "113d48a9071240f3a697edcd63bb33c2",
            "253d0fb8003e4200911e26c5d71a01f4",
            "7981d9aa530b4d4d9432e9cb445f4de8",
            "5a79f0a017bc4cd6bb58175f3dd3c60e",
            "662ec579b98142eb81a167c44e3a13df",
            "6f69fe7cd4534154a84076e3b08f3e0c",
            "e18ab399ecce416390b8236c06e59592",
            "535dbac28ad746afacfac20ea6e82dac",
            "8eeaf9cd5b024132a79ea8596f762cd8"
          ]
        },
        "outputId": "af8d9d8c-bdc4-4539-cd9d-2cc4352c225e"
      },
      "source": [
        "# Model configuration augments\n",
        "parser = Trainer.add_argparse_args(parser)\n",
        "args = parser.parse_args(args=['--train', '--dataset_path', '/content/drive/My Drive/KoGPT2-personachat/dataset/sample.json'])\n",
        "\n",
        "tokenizer, detokenizer, vocab = get_kogpt2_tokenizer()\n",
        "model = CMPersonaChat(args)\n",
        "model.to(args.device)\n",
        "\n",
        "# Fine-tuning KoGPT2 for the PersonaChat\n",
        "train_loader, val_loader = get_data_loaders(args, tokenizer, vocab)\n",
        "tb_logger = TensorBoardLogger(\"/content/drive/My Drive/KoGPT2-personachat/logs\", name=args.name)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='{}/checkpoints/{}'.format(tb_logger.log_dir, '{epoch:02d}-{val_loss:.4f}'),\n",
        "    verbose=True,\n",
        "    save_last=True,\n",
        "    save_top_k=10,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    prefix='model_'\n",
        ")\n",
        "\n",
        "if args.restore:\n",
        "    trainer = Trainer(resume_from_checkpoint=args.model_params,\n",
        "                        checkpoint_callback=checkpoint_callback,\n",
        "                        gradient_clip_val=1.0,\n",
        "                        logger=tb_logger)\n",
        "else:\n",
        "    trainer = Trainer.from_argparse_args(\n",
        "        args,\n",
        "        checkpoint_callback=checkpoint_callback,\n",
        "        weights_save_path=os.getcwd(),\n",
        "        gradient_clip_val=1.0,\n",
        "        logger=tb_logger)\n",
        "model.train()\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "logging.info('best model path {}'.format(checkpoint_callback.best_model_path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:cm_kogpt2:Load tokenized dataset from cache at dataset_cache_sample\n",
            "INFO:cm_kogpt2:Build inputs and labels\n",
            "INFO:cm_kogpt2:Pad inputs and convert to Tensor\n",
            "INFO:cm_kogpt2:Build train and validation dataloaders\n",
            "GPU available: True, used: False\n",
            "INFO:lightning:GPU available: True, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "INFO:lightning:TPU available: False, using: 0 TPU cores\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "  | Name   | Type            | Params\n",
            "-------------------------------------------\n",
            "0 | kogpt2 | GPT2LMHeadModel | 124 M \n",
            "INFO:lightning:\n",
            "  | Name   | Type            | Params\n",
            "-------------------------------------------\n",
            "0 | kogpt2 | GPT2LMHeadModel | 124 M \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a35ed21a3b8410385c4af3fc0dc976d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "975ab6a2699942b3b542e32f5a558526",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30c62ef5a7f146749f3b629b762ad744",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00000: val_loss reached 4.03494 (best 4.03494), saving model to /content/drive/My Drive/KoGPT2-personachat/logs/cm_kogpt2/version_1/checkpoints/model_epoch=00-val_loss=4.0349.ckpt as top 10\n",
            "INFO:lightning:\n",
            "Epoch 00000: val_loss reached 4.03494 (best 4.03494), saving model to /content/drive/My Drive/KoGPT2-personachat/logs/cm_kogpt2/version_1/checkpoints/model_epoch=00-val_loss=4.0349.ckpt as top 10\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "253d0fb8003e4200911e26c5d71a01f4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}